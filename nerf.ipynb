{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三维重建之神经辐射场 (NeRF)\n",
    "\n",
    "在这个例子中，我们展示了研究论文 [NeRF](https://arxiv.org/abs/2003.08934) 的最小实现：将场景表示为 Ben Mildenhall 等人的视图合成的神经辐射场。作者提出了一种巧妙的方法，通过神经网络对体积场景函数进行建模，从而合成场景的新视图。\n",
    "\n",
    "为了帮助您直观地理解这一点，让我们从以下问题开始：是否可以将图像中像素的位置提供给神经网络，并要求网络预测该位置的颜色？\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/nerf_pipeline_1.png\" width=\"500\" alt=\"nerf_pipeline_1\">\n",
    "<figcaption>图 1：给定图像坐标的神经网络作为输入并要求预测坐标处的颜色。</figcaption>\n",
    "</figure>\n",
    "\n",
    "神经网络会假设记忆（过拟合）图像。这意味着我们的神经网络会将整个图像编码为其权重。我们可以用每个位置查询神经网络，它最终会重建整个图像。\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/nerf_pipeline_2.png\" width=\"500\" alt=\"nerf_pipeline_1\">\n",
    "<figcaption>图 2：经过训练的神经网络从头开始重新创建图像。</figcaption>\n",
    "</figure>\n",
    "\n",
    "现在出现了一个问题，我们如何扩展这个想法来学习 3D 体积场景？实现与上述类似的过程需要了解每个体素（体积像素）。事实证明，这是一项非常具有挑战性的任务。\n",
    "\n",
    "该论文的作者提出了一种使用场景的一些图像来学习 3D 场景的最小而优雅的方法。他们放弃使用体素进行训练。网络学习对体积场景进行建模，从而生成模型在训练时未显示的 3D 场景的新视图（图像）。\n",
    "\n",
    "需要了解一些先决条件才能充分理解这一过程。我们以这样一种方式构建示例，以便您在开始实施之前拥有所有必需的知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GLOG_v\"] = \"3\"\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mindspore\n",
    "import mindspore as md\n",
    "import mindspore.ops.operations as P\n",
    "from mindspore import nn, ops, Tensor\n",
    "import mindspore as md\n",
    "\n",
    "\n",
    "# environment config\n",
    "mode = \"GRAPH_MODE\"\n",
    "seed = 1\n",
    "device_id = 0\n",
    "device = \"GPU\"\n",
    "init = False\n",
    "\n",
    "# data config\n",
    "half_res = True\n",
    "testskip = 8\n",
    "white_bkgd = True\n",
    "render_test = True\n",
    "\n",
    "# network config\n",
    "chunk = 32768\n",
    "cap_n_samples = 64\n",
    "cap_n_importance = 0\n",
    "netchunk = 65536\n",
    "lrate = 0.0005\n",
    "\n",
    "# runner config\n",
    "cap_n_iters = 3000\n",
    "cap_n_rand = 1024\n",
    "i_testset = 100\n",
    "lrate_decay = 250\n",
    "\n",
    "\n",
    "def context_setup(idx, device='GPU', mode=md.context.GRAPH_MODE):\n",
    "    if init:\n",
    "        return\n",
    "    if device == \"CPU\":\n",
    "        raise NotImplementedError(\"`cumprod` ops does not support CPU\")\n",
    "    md.context.set_context(mode=mode, device_target=device, device_id=idx)\n",
    "\n",
    "md.set_seed(seed)\n",
    "context_setup(device_id, device, getattr(md.context, mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载并加载数据\n",
    "\n",
    "### 预备知识\n",
    "\n",
    "数据文件包含图像、相机姿势和焦距。这些图像是从多个摄像机角度拍摄的，如图 3 所示。\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/nerf_data_1.png\" width=\"500\" alt=\"nerf_pipeline_1\">\n",
    "<figcaption>图 3：多个摄像机角度。</figcaption>\n",
    "</figure>\n",
    "\n",
    "要在这种情况下理解相机姿势，我们必须首先让自己认为相机是现实世界和二维图像之间的映射。\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/nerf_data_2.png\" width=\"500\" alt=\"nerf_pipeline_1\">\n",
    "<figcaption>图 4：通过相机将 3-D 世界映射到 2-D 图像。</figcaption>\n",
    "</figure>\n",
    "\n",
    "考虑以下等式：\n",
    "\n",
    "$$\n",
    "x = PX\n",
    "$$\n",
    "\n",
    "其中 x 是 2-D 图像点，X 是 3-D 世界点，P 是相机矩阵。P是 一个 3 x 4 矩阵，在将现实世界对象映射到图像平面上起着至关重要的作用。\n",
    "\n",
    "相机矩阵是一个仿射变换矩阵，它与一个 3 x 1 列连接 `[image height, image width, focal length]`以产生姿势矩阵。该矩阵的尺寸为 3 x 5，其中第一个 3 x 3 块位于相机的视点中。轴是 `[down, right, backwards]` 或 `[-y, x, z]` 相机面向前方的位置-z。\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/nerf_data_3.png\" width=\"500\" alt=\"nerf_pipeline_1\">\n",
    "<figcaption>图 5：仿射变换。</figcaption>\n",
    "</figure>\n",
    "\n",
    "COLMAP 帧是 `[right, down, forwards]` 或 `[x, -y, -z]`。[在此处](https://colmap.github.io/)阅读有关 COLMAP 的更多信息。\n",
    "\n",
    "### 下载数据\n",
    "\n",
    "首先自行下载数据: [NeRF Synthetics](https://drive.google.com/file/d/18JxhpWD-4ZmuFKLzKlAw-w5PpzZxXOcG/view?usp=sharing).\n",
    "\n",
    "下载数据后, 按照以下结构安排数据:\n",
    "\n",
    "```text\n",
    ".datasets/\n",
    "└── nerf_synthetics\n",
    "    └── lego\n",
    "        ├── test [600 entries exceeds filelimit, not opening dir]\n",
    "        ├── train [100 entries exceeds filelimit, not opening dir]\n",
    "        ├── transforms_test.json\n",
    "        ├── transforms_train.json\n",
    "        ├── transforms_val.json\n",
    "        └── val [100 entries exceeds filelimit, not opening dir]\n",
    "```\n",
    "\n",
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = Path(\"datasets/nerf_synthetics/lego\")\n",
    "train_imgs = list((datadir / \"train\").glob(\"*.png\"))\n",
    "num_images = len(train_imgs)\n",
    "\n",
    "temp_fname = str(train_imgs[np.random.randint(low=0, high=num_images)])\n",
    "tmp_img = cv2.imread(temp_fname, cv2.IMREAD_UNCHANGED)\n",
    "tmp_img = cv2.cvtColor(tmp_img, cv2.COLOR_BGRA2RGBA)\n",
    "\n",
    "plt.imshow(tmp_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据管线\n",
    "\n",
    "现在您已经了解了相机矩阵的概念以及从 3D 场景到 2D 图像的映射，让我们来谈谈逆映射，即从 2D 图像到 3D 场景。\n",
    "\n",
    "我们需要讨论使用光线投射和追踪的体积渲染，这是常见的计算机图形技术。本节将帮助您快速掌握这些技术。\n",
    "\n",
    "考虑一个带有 `N` 像素的图像。我们通过每个像素射出一条射线，并在射线上采样一些点。射线通常由方程参数化，`r(t) = o + td` 其中 `t` 是参数，`o` 是原点并且 `d` 是单位方向矢量。\n",
    "\n",
    "我们考虑一条射线，并在射线上采样一些随机点。这些采样点每个都有一个独特的位置`(x, y, z)`，并且光​​线有一个视角`(theta, phi)`。视角特别有趣，因为我们可以通过许多不同的方式通过单个像素拍摄光线，每种方式都有独特的视角。这里要注意的另一件有趣的事情是添加到采样过程中的噪声。我们为每个样本添加均匀的噪声，使样本对应于连续分布。这些采样点作为 NeRF 模型的输入。然后要求模型预测该点的 RGB 颜色和体积密度。\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/nerf_data_pipeline.png\" width=\"500\" alt=\"nerf_pipeline_1\">\n",
    "<figcaption>图 6：数据管线</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_t(t):\n",
    "    return np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, t], [0, 0, 0, 1]],\n",
    "                    dtype=np.float32)\n",
    "\n",
    "\n",
    "def rot_phi(phi):\n",
    "    return np.array([[1, 0, 0, 0], [0, np.cos(phi), -np.sin(phi), 0],\n",
    "                     [0, np.sin(phi), np.cos(phi), 0], [0, 0, 0, 1]],\n",
    "                    dtype=np.float32)\n",
    "\n",
    "\n",
    "def rot_theta(th):\n",
    "    return np.array([[np.cos(th), 0, -np.sin(th), 0], [0, 1, 0, 0],\n",
    "                     [np.sin(th), 0, np.cos(th), 0], [0, 0, 0, 1]],\n",
    "                    dtype=np.float32)\n",
    "\n",
    "\n",
    "def pose_spherical(theta, phi, radius):\n",
    "    \"\"\"pose_spherical\"\"\"\n",
    "    c2w = trans_t(radius)\n",
    "    c2w = np.matmul(rot_phi(phi / 180. * np.pi), c2w)\n",
    "    c2w = np.matmul(rot_theta(theta / 180. * np.pi), c2w)\n",
    "    c2w = np.matmul(\n",
    "        np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]],\n",
    "                 dtype=np.float32), c2w)\n",
    "    return c2w\n",
    "\n",
    "\n",
    "def load_blender_data(basedir, half_res=False, testskip=1):\n",
    "    \"\"\"load_blender_data\"\"\"\n",
    "    splits = ['train', 'val', 'test']\n",
    "    metas = {}\n",
    "    for s in splits:\n",
    "        with open(os.path.join(basedir, f'transforms_{s}.json'),\n",
    "                  'r',\n",
    "                  encoding='utf-8') as fp:\n",
    "            metas[s] = json.load(fp)\n",
    "\n",
    "    all_imgs = []\n",
    "    all_poses = []\n",
    "    counts = [0]\n",
    "    for s in splits:\n",
    "        meta = metas[s]\n",
    "        imgs = []\n",
    "        poses = []\n",
    "        if s == 'train' or testskip == 0:\n",
    "            skip = 1\n",
    "        else:\n",
    "            skip = testskip\n",
    "\n",
    "        for frame in meta['frames'][::skip]:\n",
    "            fname = os.path.join(basedir, frame['file_path'] + '.png')\n",
    "            tmp_img = cv2.imread(fname, cv2.IMREAD_UNCHANGED)\n",
    "            tmp_img = cv2.cvtColor(tmp_img, cv2.COLOR_BGRA2RGBA)\n",
    "            imgs.append(tmp_img)\n",
    "            poses.append(np.array(frame['transform_matrix']))\n",
    "        imgs = (np.array(imgs) / 255.).astype(\n",
    "            np.float32)  # keep all 4 channels (RGBA)\n",
    "        poses = np.array(poses).astype(np.float32)\n",
    "        counts.append(counts[-1] + imgs.shape[0])\n",
    "        all_imgs.append(imgs)\n",
    "        all_poses.append(poses)\n",
    "\n",
    "    i_split = [np.arange(counts[i], counts[i + 1]) for i in range(3)]\n",
    "\n",
    "    imgs = np.concatenate(all_imgs, 0)\n",
    "    poses = np.concatenate(all_poses, 0)\n",
    "\n",
    "    cap_h, cap_w = imgs[0].shape[:2]\n",
    "    camera_angle_x = float(meta['camera_angle_x'])\n",
    "    focal = .5 * cap_w / np.tan(.5 * camera_angle_x)\n",
    "\n",
    "    render_poses = np.stack([\n",
    "        pose_spherical(angle, -30.0, 4.0)\n",
    "        for angle in np.linspace(-180, 180, 40 + 1)[:-1]\n",
    "    ], axis=0)\n",
    "\n",
    "    if half_res:\n",
    "        cap_h = cap_h // 8\n",
    "        cap_w = cap_w // 8\n",
    "        focal = focal / 8.\n",
    "\n",
    "        imgs_half_res = np.zeros((imgs.shape[0], cap_h, cap_w, 4))\n",
    "        for i, img in enumerate(imgs):\n",
    "            imgs_half_res[i] = cv2.resize(img, (cap_h, cap_w),\n",
    "                                          interpolation=cv2.INTER_AREA)\n",
    "        imgs = imgs_half_res\n",
    "\n",
    "    return md.Tensor(imgs).astype(\"float32\"), md.Tensor(poses).astype(\n",
    "        \"float32\"), md.Tensor(render_poses).astype(\"float32\"), [\n",
    "            cap_h, cap_w, focal\n",
    "        ], i_split\n",
    "\n",
    "\n",
    "images, poses, render_poses, hwf, i_split = load_blender_data(datadir, half_res, testskip)\n",
    "print('Loaded blender', images.shape, render_poses.shape, hwf, datadir)\n",
    "i_train, i_val, i_test = i_split\n",
    "\n",
    "near = 2.\n",
    "far = 6.\n",
    "\n",
    "if white_bkgd:\n",
    "    images = images[..., :3] * images[..., -1:] + (1. - images[..., -1:])\n",
    "else:\n",
    "    images = images[..., :3]\n",
    "\n",
    "if render_test:\n",
    "    render_poses = poses[i_test.tolist()]\n",
    "\n",
    "# Cast intrinsics to right types\n",
    "cap_h, cap_w, focal = hwf\n",
    "cap_h, cap_w = int(cap_h), int(cap_w)\n",
    "\n",
    "hwf = [cap_h, cap_w, focal]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeRF 模型\n",
    "\n",
    "该模型是一个多层感知器 (MLP)，以 ReLU 作为其非线性。\n",
    "\n",
    "论文摘录：\n",
    "\n",
    "“我们通过限制网络将体积密度 sigma 预测为仅位置的函数来鼓励表示是多视图一致的 x，同时允许将 RGB 颜色c预测为位置和查看方向的函数。为此， MLP 首先使用 8 个全连接层（使用 ReLU 激活和每层 256 个通道）处理输入 3D 坐标，并输出 sigma 和 256 维特征向量。然后将该特征向量与相机光线的观察方向连接并传递到一个额外的全连接层（使用 ReLU 激活和 128 个通道），输出与视图相关的 RGB 颜色。”\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/nerf_model.png\" width=\"500\" alt=\"nerf_pipeline_1\">\n",
    "<figcaption>图 7：NeRF 模型</figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRFMLP(nn.Cell):\n",
    "    \"\"\"\n",
    "    NeRF MLP architecture.\n",
    "\n",
    "    Args:\n",
    "        cap_d (int, optional): Model depth. Default: 8.\n",
    "        cap_w (int, optional): Model width. Default: 256.\n",
    "        input_ch (int, optional): Input channel. Default: 3.\n",
    "        input_ch_views (int, optional): Input view channel. Default: 3.\n",
    "        output_ch (int, optional): Output channel. Default: 4.\n",
    "        skips (tuple, optional): Skip connection layer index. Default: (4).\n",
    "        use_view_dirs (bool, optional): Use view directions or not. Default: False.\n",
    "\n",
    "    Inputs:\n",
    "        - **x** (Tensor) - Query tensors. points and view directions (..., 6).\n",
    "\n",
    "    Outputs:\n",
    "        Tensor, query features (..., feature_dims).\n",
    "\n",
    "    Supported Platforms:\n",
    "        ``Ascend`` ``GPU`` ``CPU``\n",
    "\n",
    "    Examples:\n",
    "        >>> model = NeRFMLP()\n",
    "        >>> inputs = ms.numpy.randn(1, 3)\n",
    "        >>> outputs = model(inputs)\n",
    "        [[0.2384 0.8456 0.6845 0.1584]]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            cap_d=8,\n",
    "            cap_w=256,\n",
    "            input_ch=3,\n",
    "            input_ch_views=3,\n",
    "            output_ch=4,\n",
    "            skips=(4),\n",
    "            use_view_dirs=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cap_d = cap_d\n",
    "        self.cap_w = cap_w\n",
    "        self.input_ch = input_ch\n",
    "        self.input_ch_views = input_ch_views\n",
    "        self.skips = skips\n",
    "        self.use_view_dirs = use_view_dirs\n",
    "\n",
    "        self.pts_linears = nn.CellList([nn.Dense(in_channels=input_ch, out_channels=cap_w)] + [\n",
    "            nn.Dense(in_channels=cap_w, out_channels=cap_w) if i not in\n",
    "            self.skips else nn.Dense(in_channels=cap_w + input_ch, out_channels=cap_w) for i in range(cap_d - 1)\n",
    "        ])\n",
    "\n",
    "        self.views_linears = nn.CellList([nn.Dense(in_channels=input_ch_views + cap_w, out_channels=cap_w // 2)])\n",
    "\n",
    "        if use_view_dirs:\n",
    "            self.feature_linear = nn.Dense(in_channels=cap_w, out_channels=cap_w)\n",
    "            self.alpha_linear = nn.Dense(in_channels=cap_w, out_channels=1)\n",
    "            self.rgb_linear = nn.Dense(in_channels=cap_w // 2, out_channels=3)\n",
    "        else:\n",
    "            self.output_linear = nn.Dense(in_channels=cap_w, out_channels=output_ch)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"NeRF MLP construct\"\"\"\n",
    "        input_pts, input_views = x[..., :self.input_ch], x[..., self.input_ch:]\n",
    "        h = input_pts\n",
    "        for i, _ in enumerate(self.pts_linears):\n",
    "            h = self.pts_linears[i](h)\n",
    "            h = P.ReLU()(h)\n",
    "            if i in self.skips:\n",
    "                h = P.Concat(-1)([input_pts, h])\n",
    "\n",
    "        if self.use_view_dirs:\n",
    "            alpha = self.alpha_linear(h)\n",
    "            feature = self.feature_linear(h)\n",
    "            h = P.Concat(-1)([feature, input_views])\n",
    "\n",
    "            for i, _ in enumerate(self.views_linears):\n",
    "                h = self.views_linears[i](h)\n",
    "                h = P.ReLU()(h)\n",
    "\n",
    "            rgb = self.rgb_linear(h)\n",
    "            outputs = P.Concat(-1)([rgb, alpha])\n",
    "        else:\n",
    "            outputs = self.output_linear(h)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可微分渲染器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumeRenderer(nn.Cell):\n",
    "    \"\"\"\n",
    "    Volume Renderer architecture.\n",
    "\n",
    "    Args:\n",
    "        chunk (int): Number of rays processed in parallel, decrease if running out of memory.\n",
    "        cap_n_samples (int): Number of coarse samples per ray for coarse net.\n",
    "        cap_n_importance (int): Number of additional fine samples per ray for fine net.\n",
    "        net_chunk (int): Number of pts sent through network in parallel, decrease if running out of memory.\n",
    "        white_bkgd (bool): Set to render synthetic data on a white background (always use for DeepVoxels).\n",
    "        model_coarse (nn.Cell): Coarse net.\n",
    "        model_fine (nn.Cell, optional): Fine net, or None.\n",
    "        embedder_p (Dict): Config for positional encoding for point.\n",
    "        embedder_d (Dict): Config for positional encoding for view direction.\n",
    "        near (float, optional): The near plane. Default: 0.0.\n",
    "        far (float, optional): The far plane. Default: 1e6.\n",
    "\n",
    "    Inputs:\n",
    "        - **rays** (Tensor) - The ray tensor. (..., num_pts_per_ray, ray_batch_dims).\n",
    "\n",
    "    Outputs:\n",
    "        Tuple of 2 Tensor, the output tensors.\n",
    "\n",
    "        - **fine_net_output** (Tensor, optional) - The fine net output features.\n",
    "        - **coarse_net_output** (Tensor) - The coarse net output features.\n",
    "\n",
    "    Supported Platforms:\n",
    "        ``Ascend`` ``GPU`` ``CPU``\n",
    "\n",
    "    Examples:\n",
    "        >>> model = VolumeRenderer(1000, 6, 12, 1000, False, P.Identity(), P.Identity(), P.Identity(), P.Identity())\n",
    "        >>> inputs = ms.numpy.randn(1, 1, 3)\n",
    "        >>> outputs = model(inputs)\n",
    "        ([[0.2384 0.8456 0.1273]], [[0.8653 0.1866 0.6382]])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 chunk,\n",
    "                 cap_n_samples,\n",
    "                 cap_n_importance,\n",
    "                 net_chunk,\n",
    "                 white_bkgd,\n",
    "                 model_coarse,\n",
    "                 model_fine,\n",
    "                 embedder_p,\n",
    "                 embedder_d,\n",
    "                 near=0.0,\n",
    "                 far=1e6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.chunk = chunk\n",
    "        self.cap_n_samples = cap_n_samples\n",
    "        self.cap_n_importance = cap_n_importance\n",
    "        self.net_chunk = net_chunk\n",
    "        self.white_bkgd = white_bkgd\n",
    "\n",
    "        self.model_coarse = model_coarse\n",
    "        self.model_fine = model_fine\n",
    "        # embedder for positions\n",
    "        self.embedder_p = Embedder(**embedder_p)\n",
    "        # embedder for view-in directions\n",
    "        self.embedder_d = Embedder(**embedder_d)\n",
    "\n",
    "        self.near = near\n",
    "        self.far = far\n",
    "\n",
    "    def construct(self, rays):\n",
    "        \"\"\"Volume renderer construct.\"\"\"\n",
    "        return self.inference(rays)\n",
    "\n",
    "    def inference(self, rays):\n",
    "        \"\"\"Volume renderer inference.\"\"\"\n",
    "        # make the number of rays be multiple of the chunk size\n",
    "        cap_n_rays = (rays.shape[1] // self.chunk + 1) * self.chunk\n",
    "        cap_n = self.cap_n_samples\n",
    "\n",
    "        res_ls = {\"rgb_map_coarse\": [], \"rgb_map_fine\": []}\n",
    "\n",
    "        for i in range(0, cap_n_rays, self.chunk):\n",
    "            ray_origins, ray_dirs = rays[:, i:i + self.chunk, :]\n",
    "            reshape_op = mindspore.ops.Reshape()\n",
    "            view_dirs = reshape_op(\n",
    "                ray_dirs / mindspore.numpy.norm(ray_dirs, axis=-1, keepdims=True),\n",
    "                (-1, 3),\n",
    "            )\n",
    "\n",
    "            near, far = self.near * mindspore.numpy.ones_like(ray_dirs[..., :1]), self.far * mindspore.numpy.ones_like(\n",
    "                ray_dirs[..., :1])\n",
    "            cap_m = ray_origins.shape[0]\n",
    "            if cap_m == 0:\n",
    "                continue\n",
    "\n",
    "            # stratified sampling along rays\n",
    "            s_samples = sample_along_rays(near, far, cap_n)\n",
    "\n",
    "            # position samples along rays\n",
    "            unsqueeze_op = P.ExpandDims()\n",
    "            pos_samples = unsqueeze_op(ray_origins,\n",
    "                                       1) + unsqueeze_op(ray_dirs, 1) * unsqueeze_op(s_samples, 2)\n",
    "            # expand ray directions to the same shape of samples\n",
    "            expand_op = P.BroadcastTo(pos_samples.shape)\n",
    "            dir_samples = expand_op(unsqueeze_op(view_dirs, 1))\n",
    "\n",
    "            reshape_op = P.Reshape()\n",
    "            pos_samples = reshape_op(pos_samples, (-1, 3))\n",
    "            dir_samples = reshape_op(dir_samples, (-1, 3))\n",
    "\n",
    "            # retrieve optic data from the network\n",
    "            optic_d = self._run_network_model_coarse(pos_samples, dir_samples)\n",
    "            optic_d = mindspore.numpy.reshape(optic_d, [cap_m, cap_n, 4])\n",
    "\n",
    "            # composite optic data to generate a RGB image\n",
    "            rgb_map_coarse, weights_coarse = self._composite(optic_d, s_samples, ray_dirs)\n",
    "\n",
    "            if self.cap_n_importance > 0:\n",
    "                z_vals_mid = 0.5 * (s_samples[..., 1:] + s_samples[..., :-1])\n",
    "                z_samples = sample_pdf(z_vals_mid, weights_coarse[..., 1:-1], self.cap_n_importance)\n",
    "                z_samples = mindspore.ops.stop_gradient(z_samples)\n",
    "\n",
    "                sort_op = P.Sort(axis=-1)\n",
    "                z_vals, _ = sort_op(P.Concat(-1)([s_samples, z_samples]))\n",
    "                pts = (ray_origins[..., None, :] + ray_dirs[..., None, :] * z_vals[..., :, None]\n",
    "                      )\n",
    "\n",
    "                expand_op_2 = P.BroadcastTo(pts.shape)\n",
    "                dir_samples = expand_op_2(unsqueeze_op(view_dirs, 1))\n",
    "\n",
    "                pts = reshape_op(pts, (-1, 3))\n",
    "                dir_samples = reshape_op(dir_samples, (-1, 3))\n",
    "\n",
    "                optic_d = self._run_network_model_fine(pts, dir_samples)\n",
    "                optic_d = reshape_op(optic_d, (cap_m, cap_n + self.cap_n_importance, 4))\n",
    "\n",
    "                rgb_map_fine, _ = self._composite(optic_d, z_vals, ray_dirs)\n",
    "            else:\n",
    "                rgb_map_fine = rgb_map_coarse\n",
    "\n",
    "            res_ls[\"rgb_map_coarse\"].append(rgb_map_coarse)\n",
    "            res_ls[\"rgb_map_fine\"].append(rgb_map_fine)\n",
    "\n",
    "        res = {}\n",
    "        for k, v in res_ls.items():\n",
    "            res[k] = P.Concat(0)(v)\n",
    "\n",
    "        return res[\"rgb_map_fine\"], res[\"rgb_map_coarse\"]\n",
    "\n",
    "    def _run_network_model_fine(self, pts, view_dirs):\n",
    "        \"\"\"Run fine model.\"\"\"\n",
    "        inputs_flat = pts\n",
    "        embedded = self.embedder_p(inputs_flat)\n",
    "\n",
    "        if view_dirs is not None:\n",
    "            input_dirs_flat = view_dirs\n",
    "            embedded_dirs = self.embedder_d(input_dirs_flat)\n",
    "            embedded = P.Concat(-1)([embedded, embedded_dirs])\n",
    "\n",
    "        chunk = self.net_chunk\n",
    "        outputs_flat_ls = []\n",
    "        for i in range(0, embedded.shape[0], chunk):\n",
    "            outputs_flat_ls.append(self.model_fine(embedded[i:i + chunk]))\n",
    "        outputs_flat = P.Concat(0)(outputs_flat_ls)\n",
    "        return outputs_flat\n",
    "\n",
    "    def _run_network_model_coarse(self, pts, view_dirs):\n",
    "        \"\"\"Run coarse model.\"\"\"\n",
    "        inputs_flat = pts\n",
    "        embedded = self.embedder_p(inputs_flat)\n",
    "\n",
    "        if view_dirs is not None:\n",
    "            input_dirs_flat = view_dirs\n",
    "            embedded_dirs = self.embedder_d(input_dirs_flat)\n",
    "            embedded = P.Concat(-1)([embedded, embedded_dirs])\n",
    "\n",
    "        chunk = self.net_chunk\n",
    "        outputs_flat_ls = []\n",
    "        for i in range(0, embedded.shape[0], chunk):\n",
    "            outputs_flat_ls.append(self.model_coarse(embedded[i:i + chunk]))\n",
    "        outputs_flat = P.Concat(0)(outputs_flat_ls)\n",
    "        return outputs_flat\n",
    "\n",
    "    def _transfer(self, optic_d, dists):\n",
    "        \"\"\"Transfer occupancy to alpha values.\"\"\"\n",
    "        sigmoid = P.Sigmoid()\n",
    "        rgbs = sigmoid(optic_d[..., :3])\n",
    "        alphas = 1.0 - P.Exp()(-1.0 * (P.ReLU()(optic_d[(..., 3)])) * dists)\n",
    "\n",
    "        return rgbs, alphas\n",
    "\n",
    "    def _composite(self, optic_d, s_samples, rays_d):\n",
    "        \"\"\"Composite the colors and densities.\"\"\"\n",
    "        # distances between each samples\n",
    "        dists = s_samples[..., 1:] - s_samples[..., :-1]\n",
    "        dists_list = (\n",
    "            dists,\n",
    "            (mindspore.numpy.ones([]) * 1e10).expand_as(dists[..., :1]),\n",
    "        )\n",
    "        dists = P.Concat(-1)(dists_list)\n",
    "\n",
    "        dists = dists * mindspore.numpy.norm(rays_d[..., None, :], axis=-1)\n",
    "\n",
    "        # retrieve display colors and alphas for each samples by a transfer function\n",
    "        rgbs, alphas = self._transfer(optic_d, dists)\n",
    "\n",
    "        weights = alphas * mindspore.numpy.cumprod(\n",
    "            P.Concat(-1)([mindspore.numpy.ones((alphas.shape[0], 1)), 1.0 - alphas + 1e-10])[:, :-1],\n",
    "            axis=-1,\n",
    "        )\n",
    "        sum_op = mindspore.ops.ReduceSum()\n",
    "        rgb_map = sum_op(weights[..., None] * rgbs, -2)\n",
    "        acc_map = sum_op(weights, -1)\n",
    "\n",
    "        if self.white_bkgd:\n",
    "            rgb_map = rgb_map + (1.0 - acc_map[..., None])\n",
    "\n",
    "        return rgb_map, weights\n",
    "\n",
    "class Embedder(nn.Cell):\n",
    "    \"\"\"\n",
    "    Embedder for positional embedding.\n",
    "\n",
    "    Args:\n",
    "        input_dims (int): Input dimensions.\n",
    "        max_freq_pow (float): Maximum frequency pow.\n",
    "        num_freqs (int): Number of frequencies.\n",
    "        periodic_fns (list, optional): Periodic fns. Default: [mindspore.ops.Sin(), mindspore.ops.Cos()].\n",
    "        log_sampling (bool, optional): Log sampling. Default: True.\n",
    "        include_input (bool, optional): Include input or not. Default: True.\n",
    "\n",
    "    Inputs:\n",
    "        inputs (Tensor) - Input tensor.\n",
    "\n",
    "    Outputs:\n",
    "        Tensor, input concatenated with positional embeddings.\n",
    "\n",
    "    Supported Platforms:\n",
    "        ``Ascend`` ``GPU`` ``CPU``\n",
    "\n",
    "    Examples:\n",
    "        >>> model = Embedder(1, 1)\n",
    "        >>> inputs = ms.numpy.randn(1)\n",
    "        >>> outputs = model(inputs)\n",
    "        [0.1384 0.4426]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dims,\n",
    "            max_freq_pow,\n",
    "            num_freqs,\n",
    "            periodic_fns=(mindspore.ops.Sin(), mindspore.ops.Cos()),\n",
    "            log_sampling=True,\n",
    "            include_input=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        embed_fns = []\n",
    "        out_dims = 0\n",
    "        if include_input:\n",
    "            embed_fns.append(mindspore.ops.Identity())\n",
    "            out_dims += input_dims\n",
    "\n",
    "        if log_sampling:\n",
    "            freq_bands = mindspore.Tensor(2.0)**mindspore.numpy.linspace(0.0, max_freq_pow, num=num_freqs)\n",
    "        else:\n",
    "            freq_bands = mindspore.numpy.linspace(2.0**0.0, 2.0**max_freq_pow, num=num_freqs)\n",
    "\n",
    "        for _ in freq_bands:\n",
    "            for p_fn in periodic_fns:\n",
    "                embed_fns.append(p_fn)\n",
    "                out_dims += input_dims\n",
    "\n",
    "        self.embed_fns = embed_fns\n",
    "        self.out_dims = out_dims\n",
    "\n",
    "        self.freq_bands = freq_bands\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        \"\"\"Embedder construct.\"\"\"\n",
    "        out = []\n",
    "        for i, fn in enumerate(self.embed_fns):\n",
    "            if i == 0:\n",
    "                out.append(fn(inputs))\n",
    "            else:\n",
    "                out.append(fn(inputs * self.freq_bands[(i - 1) // 2]))\n",
    "        return P.Concat(-1)(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedder(multi_res, i=0):\n",
    "    \"\"\"\n",
    "    Get embedder function.\n",
    "\n",
    "    Args:\n",
    "        multi_res (int): Log2 of max freq for positional encoding.\n",
    "        i (int, optional): Set 0 for default positional encoding, -1 for none. Default: 0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of nn.Cell and int, embedder and the output dimensions.\n",
    "\n",
    "        - **embedder** (nn.Cell) - The embedder.\n",
    "        - **out_dims** (int) - The output dimensions.\n",
    "    \"\"\"\n",
    "    if i == -1:\n",
    "        return md.ops.Identity(), 3\n",
    "\n",
    "    embed_kwargs = {\n",
    "        \"include_input\": True,\n",
    "        \"input_dims\": 3,\n",
    "        \"max_freq_pow\": multi_res - 1,\n",
    "        \"num_freqs\": multi_res,\n",
    "        \"log_sampling\": True,\n",
    "        \"periodic_fns\": [md.ops.Sin(), md.ops.Cos()],\n",
    "    }\n",
    "\n",
    "    embedder_obj = Embedder(**embed_kwargs)\n",
    "    embed = embed_kwargs\n",
    "    return embed, embedder_obj.out_dims\n",
    "\n",
    "\n",
    "def create_nerf(multires=10,\n",
    "                i_embed=0,\n",
    "                multires_views=4,\n",
    "                netdepth=8,\n",
    "                netwidth=256,\n",
    "                netdepth_fine=8,\n",
    "                netwidth_fine=256,\n",
    "                use_view_dirs=True,\n",
    "                cap_n_importance=0,\n",
    "                ckpt_path=None):\n",
    "    \"\"\"create nerf model and load weights\"\"\"\n",
    "    embed_fn, input_ch = get_embedder(multires, i_embed)\n",
    "\n",
    "    input_ch_views = 0\n",
    "    embeddirs_fn = None\n",
    "    if use_view_dirs:\n",
    "        embeddirs_fn, input_ch_views = get_embedder(multires_views,\n",
    "                                                    i_embed)\n",
    "    # Create networks\n",
    "    output_ch = 4\n",
    "    skips = [4]\n",
    "    model_coarse = NeRFMLP(cap_d=netdepth,\n",
    "                           cap_w=netwidth,\n",
    "                           input_ch=input_ch,\n",
    "                           output_ch=output_ch,\n",
    "                           skips=skips,\n",
    "                           input_ch_views=input_ch_views,\n",
    "                           use_view_dirs=use_view_dirs)\n",
    "    grad_vars = [{\"params\": model_coarse.trainable_params()}]\n",
    "\n",
    "    model_fine = None\n",
    "    if cap_n_importance > 0:\n",
    "        model_fine = NeRFMLP(cap_d=netdepth_fine,\n",
    "                             cap_w=netwidth_fine,\n",
    "                             input_ch=input_ch,\n",
    "                             output_ch=output_ch,\n",
    "                             skips=skips,\n",
    "                             input_ch_views=input_ch_views,\n",
    "                             use_view_dirs=use_view_dirs)\n",
    "        grad_vars += [{\"params\": model_fine.trainable_params()}]\n",
    "\n",
    "    optimizer = None\n",
    "    start_iter = 0\n",
    "\n",
    "    # Load checkpoints\n",
    "    if ckpt_path is not None:\n",
    "        print(\"Reloading from\", ckpt_path)\n",
    "        ckpt = md.load_checkpoint(ckpt_path)\n",
    "\n",
    "        # Load training steps\n",
    "        start_iter = int(ckpt[\"global_steps\"]) + 1\n",
    "\n",
    "        # Load network weights\n",
    "        md.load_param_into_net(\n",
    "            model_coarse,\n",
    "            {key: value for key, value in ckpt.items() if \".model_coarse.\" in key},\n",
    "        )\n",
    "        if model_fine is not None:\n",
    "            md.load_param_into_net(\n",
    "                model_fine,\n",
    "                {key: value for key, value in ckpt.items() if \".model_fine.\" in key},\n",
    "            )\n",
    "    else:\n",
    "        print(\"No ckpt reloaded\")\n",
    "\n",
    "    return start_iter, optimizer, model_coarse, model_fine, embed_fn, embeddirs_fn\n",
    "\n",
    "class RendererWithCriterion(nn.Cell):\n",
    "    \"\"\"\n",
    "    Renderer with criterion.\n",
    "\n",
    "    Args:\n",
    "        renderer (nn.Cell): Renderer.\n",
    "        loss_fn (nn.Cell, optional): Loss function. Default: nn.MSELoss().\n",
    "\n",
    "    Inputs:\n",
    "        - **rays** (Tensor) - Rays tensor.\n",
    "        - **gt** (Tensor) - Ground truth tensor.\n",
    "\n",
    "    Outputs:\n",
    "        Tensor, loss for one forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(self, renderer, loss_fn=nn.MSELoss()):\n",
    "        \"\"\"Renderer with criterion.\"\"\"\n",
    "        super().__init__()\n",
    "        self.renderer = renderer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def construct(self, rays, gt):\n",
    "        \"\"\"Renderer Trainer construct.\"\"\"\n",
    "        rgb_map_fine, rgb_map_coarse = self.renderer(rays)\n",
    "        return self.loss_fn(rgb_map_fine, gt) + self.loss_fn(rgb_map_coarse, gt)\n",
    "\n",
    "\n",
    "# Create nerf model\n",
    "start_iter, optimizer, model_coarse, model_fine, embed_fn, embeddirs_fn = create_nerf()\n",
    "# Training steps\n",
    "global_steps = start_iter\n",
    "# Create volume renderer\n",
    "renderer = VolumeRenderer(chunk, cap_n_samples,\n",
    "                          cap_n_importance, netchunk,\n",
    "                          white_bkgd, model_coarse, model_fine,\n",
    "                          embed_fn, embeddirs_fn, near, far)\n",
    "\n",
    "renderer.model_coarse.to_float(md.dtype.float16)\n",
    "if renderer.model_fine is not None:\n",
    "    renderer.model_fine.to_float(md.dtype.float16)\n",
    "\n",
    "renderer_with_criteron = RendererWithCriterion(renderer)\n",
    "optimizer = nn.Adam(params=renderer.trainable_params(),\n",
    "                    learning_rate=lrate,\n",
    "                    beta1=0.9,\n",
    "                    beta2=0.999)\n",
    "\n",
    "grad_scale = ops.MultitypeFuncGraph(\"grad_scale\")\n",
    "@grad_scale.register(\"Tensor\", \"Tensor\")\n",
    "def gradient_scale(scale, grad):\n",
    "    return grad * ops.cast(scale, ops.dtype(grad))\n",
    "\n",
    "class CustomTrainOneStepCell(nn.TrainOneStepCell):\n",
    "    def __init__(self, network, optimizer, sens=1.0):\n",
    "        super(CustomTrainOneStepCell, self).__init__(network, optimizer, sens)\n",
    "        self.hyper_map = ops.HyperMap()\n",
    "        self.reciprocal_sense = Tensor(1 / sens, mindspore.float32)\n",
    "\n",
    "    def scale_grad(self, gradients):\n",
    "        gradients = self.hyper_map(ops.partial(grad_scale, self.reciprocal_sense), gradients)\n",
    "        return gradients\n",
    "\n",
    "    def construct(self, *inputs):\n",
    "        loss = self.network(*inputs)\n",
    "        sens = ops.fill(loss.dtype, loss.shape, self.sens)\n",
    "        # calculate gradients, the sens will equal to the loss_scale\n",
    "        grads = self.grad(self.network, self.weights)(*inputs, sens)\n",
    "        # gradients / loss_scale\n",
    "        grads = self.scale_grad(grads)\n",
    "        # reduce gradients in distributed scenarios\n",
    "        grads = self.grad_reducer(grads)\n",
    "        loss = ops.depend(loss, self.optimizer(grads))\n",
    "        return loss\n",
    "\n",
    "loss_scale = 1024.0\n",
    "train_renderer = CustomTrainOneStepCell(renderer_with_criteron, optimizer, loss_scale)\n",
    "train_renderer.set_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rays(h, w, f, pose):\n",
    "    '''\n",
    "    Given an image plane, generate rays from the camera origin to each pixel on the image plane.\n",
    "\n",
    "    Arguments:\n",
    "        h: height of the image plane.\n",
    "        w: width of the image plane.\n",
    "        f: focal length of the image plane.\n",
    "        pose: the extrinsic parameters of the camera. (3, 4) or (4, 4)\n",
    "\n",
    "    Returns:\n",
    "        A tuple: origins of rays, directions of rays\n",
    "    '''\n",
    "\n",
    "    # Coordinates of the 2D grid\n",
    "    cols = md.ops.ExpandDims()(\n",
    "        md.numpy.linspace(-1.0 * w / 2, w - 1 - w / 2, w) / f,\n",
    "        0).repeat(h, axis=0)  # (h, w)\n",
    "    rows = md.ops.ExpandDims()(\n",
    "        -1.0 * md.numpy.linspace(-1.0 * h / 2, h - 1 - h / 2, h) / f,\n",
    "        1).repeat(w, axis=1)  # (h, w)\n",
    "\n",
    "    # Ray directions for all pixels\n",
    "    ray_dirs = md.numpy.stack([cols, rows, -1.0 * md.numpy.ones_like(cols)],\n",
    "                              axis=-1)  # (h, w, 3)\n",
    "    # Apply rotation transformation to make each ray orient according to the camera\n",
    "    unsqueeze_op = md.ops.ExpandDims()\n",
    "    ray_dirs = md.numpy.sum(unsqueeze_op(ray_dirs, 2) * pose[:3, :3], axis=-1)\n",
    "    # Origin position\n",
    "    rays_oris = pose[:3, -1].expand_as(ray_dirs)  # (h, w, 3)\n",
    "\n",
    "    return rays_oris, ray_dirs.astype(pose.dtype)  # (h, w, 3), (h, w, 3)\n",
    "\n",
    "\n",
    "def train_net(iter_, train_renderer, optimizer, rays, gt):\n",
    "    '''\n",
    "    Train a network.\n",
    "\n",
    "    Arguments:\n",
    "        config: configuration.\n",
    "        iter_: current iterations.\n",
    "        renderer: a volume renderer.\n",
    "        optimizer: a network optimizer.\n",
    "        rays: a batch of rays for training. (#rays * #samples, 6)\n",
    "        gt: the groundtruth.\n",
    "\n",
    "    Returns:\n",
    "        A tuple: (MSE loss, PSNR).\n",
    "    '''\n",
    "    loss = train_renderer(rays, gt)\n",
    "\n",
    "    # Update learning rate\n",
    "    decay_rate = 0.1\n",
    "    decay_steps = lrate_decay * 1000\n",
    "    new_lrate = lrate * (decay_rate**(iter_ / decay_steps))\n",
    "    optimizer.learning_rate = md.Parameter(new_lrate)\n",
    "\n",
    "    return float(loss), float(psnr_from_mse(loss))\n",
    "\n",
    "\n",
    "def test_net(img_h,\n",
    "             img_w,\n",
    "             focal,\n",
    "             renderer,\n",
    "             test_poses,\n",
    "             gt=None,\n",
    "             on_progress=None,\n",
    "             on_complete=None):\n",
    "    '''\n",
    "    Test the network and generate results.\n",
    "\n",
    "    Arguments:\n",
    "        img_h: height of image plane.\n",
    "        img_w: width of image plane.\n",
    "        focal: focal length.\n",
    "        renderer: the volume renderer.\n",
    "        test_poses: poses used to test the network. (#poses, 4, 4)\n",
    "        on_progress: a callback function invoked per generation of a result.\n",
    "        on_complete: a callback function invoked after generating all results.\n",
    "\n",
    "    Returns:\n",
    "        A tuple: (Mean test time, MSE loss, PSNR).\n",
    "    '''\n",
    "\n",
    "    rgb_maps = []\n",
    "    loss_ls = []\n",
    "    psnr_ls = []\n",
    "    time_ls = []\n",
    "\n",
    "    reshape_op = md.ops.Reshape()\n",
    "    stack_op = md.ops.Stack(axis=0)\n",
    "\n",
    "    image_list = []\n",
    "\n",
    "    for j, test_pose in enumerate(test_poses):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Generate rays for all pixels\n",
    "        ray_oris, ray_dirs = generate_rays(img_h, img_w, focal, test_pose)\n",
    "        ray_oris = reshape_op(ray_oris, (-1, 3))\n",
    "        ray_dirs = reshape_op(ray_dirs, (-1, 3))\n",
    "        test_batch_rays = stack_op([ray_oris, ray_dirs])\n",
    "\n",
    "        # Retrieve testing results\n",
    "        rgb_map, _ = renderer.inference(test_batch_rays)\n",
    "        rgb_map = reshape_op(rgb_map, (img_h, img_w, 3))\n",
    "        rgb_maps.append(rgb_map.asnumpy())\n",
    "\n",
    "        # If given groundtruth, compute MSE and PSNR\n",
    "        if gt is not None:\n",
    "            loss = mse(rgb_map, gt[j])\n",
    "            psnr = psnr_from_mse(loss)\n",
    "            loss_ls.append(float(loss))\n",
    "            psnr_ls.append(float(psnr))\n",
    "\n",
    "        time_ls.append(time.time() - t0)\n",
    "\n",
    "        # Handle each testing result\n",
    "        if on_progress:\n",
    "            if isinstance(on_progress, list):\n",
    "                on_progress[0](j, rgb_maps[-1])\n",
    "                if gt is not None:\n",
    "                    on_progress[1](j, gt[j].asnumpy())\n",
    "            else:\n",
    "                on_progress(j, rgb_maps[-1])\n",
    "\n",
    "        image_list.append(to8b(rgb_maps[-1]))\n",
    "\n",
    "    # Handle all testing results\n",
    "    if on_complete:\n",
    "        on_complete(np.stack(rgb_maps, 0))\n",
    "\n",
    "    if not loss_ls:\n",
    "        loss_ls = [0.0]\n",
    "    if not psnr_ls:\n",
    "        psnr_ls = [0.0]\n",
    "    if not time_ls:\n",
    "        time_ls = [0.0]\n",
    "\n",
    "    return np.mean(time_ls), np.mean(loss_ls), np.mean(psnr_ls), image_list\n",
    "\n",
    "\n",
    "def to8b(x):\n",
    "    \"\"\"Convert normalized color to 8-bit color\"\"\"\n",
    "    return (255 * np.clip(x, 0.0, 1.0)).astype(np.uint8)\n",
    "\n",
    "\n",
    "def mse(im1, im2):\n",
    "    '''\n",
    "    MSE between two images.\n",
    "    '''\n",
    "\n",
    "    return md.numpy.mean((im1 - im2)**2)\n",
    "\n",
    "\n",
    "psnr_from_mse_base = md.Tensor([10.0])\n",
    "\n",
    "\n",
    "def psnr_from_mse(v):\n",
    "    '''\n",
    "    Convert MSE to PSNR.\n",
    "    '''\n",
    "    return -10.0 * (md.numpy.log(v) / md.numpy.log(psnr_from_mse_base))\n",
    "\n",
    "\n",
    "def sample_grid_2d(cap_h, cap_w, cap_n):\n",
    "    \"\"\"\n",
    "    Sample cells in an cap_h x cap_w mesh grid.\n",
    "\n",
    "    Args:\n",
    "        cap_h (int): Height of the mesh grid.\n",
    "        cap_w (int): Width of the mesh grid.\n",
    "        cap_n (int): The number of samples.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of 2 Tensor, sampled rows and sampled columns.\n",
    "\n",
    "        - **select_coords_x** (Tensor) - Sampled rows.\n",
    "        - **select_coords_y** (Tensor) - Sampled columns.\n",
    "    \"\"\"\n",
    "    if cap_n > cap_w * cap_h:\n",
    "        cap_n = cap_w * cap_h\n",
    "\n",
    "    # Create a 2D mesh grid where each element is the coordinate of the cell\n",
    "    stack_op = md.ops.Stack(-1)\n",
    "    coords = stack_op(\n",
    "        md.numpy.meshgrid(\n",
    "            md.numpy.linspace(0, cap_h - 1, cap_h),\n",
    "            md.numpy.linspace(0, cap_w - 1, cap_w),\n",
    "            indexing=\"ij\",\n",
    "        ))\n",
    "    # Flat the mesh grid\n",
    "    coords = md.ops.Reshape()(coords, (-1, 2))\n",
    "    # Sample N cells in the mesh grid\n",
    "    select_indexes = np.random.choice(coords.shape[0],\n",
    "                                      size=[cap_n],\n",
    "                                      replace=False)\n",
    "    # Sample N cells among the mesh grid\n",
    "    select_coords = coords[select_indexes.tolist()].astype(\"int32\")\n",
    "\n",
    "    return select_coords[:, 0], select_coords[:, 1]\n",
    "\n",
    "\n",
    "def sample_along_rays(near,\n",
    "                      far,\n",
    "                      cap_cap_n_samples,\n",
    "                      lin_disp=False,\n",
    "                      perturb=True):\n",
    "    \"\"\"\n",
    "    Sample points along rays.\n",
    "\n",
    "    Args:\n",
    "        near (Tensor): A vector containing nearest point for each ray. (cap_n_rays).\n",
    "        far (Tensor): A vector containing furthest point for each ray. (cap_n_rays).\n",
    "        cap_n_samples (int): The number of sampled points for each ray.\n",
    "        lin_disp (bool): True for sample linearly in inverse depth rather than in depth (used for some datasets).\n",
    "        perturb (bool): True for stratified sampling. False for uniform sampling.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, samples where j-th component of the i-th row is the j-th sampled position along the i-th ray.\n",
    "    \"\"\"\n",
    "    # The number of rays\n",
    "    cap_n_rays = near.shape[0]\n",
    "\n",
    "    # Uniform samples along rays\n",
    "    t_vals = md.numpy.linspace(0.0, 1.0, num=cap_cap_n_samples)\n",
    "    if not lin_disp:\n",
    "        z_vals = near * (1.0 - t_vals) + far * t_vals\n",
    "    else:\n",
    "        z_vals = 1.0 / (1.0 / near * (1.0 - t_vals) + 1.0 / far * t_vals)\n",
    "\n",
    "    expand_op = md.ops.BroadcastTo((cap_n_rays, cap_cap_n_samples))\n",
    "    z_vals = expand_op(z_vals)\n",
    "\n",
    "    if perturb:\n",
    "        # Get intervals between samples\n",
    "        mids = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "        cat_op = md.ops.Concat(-1)\n",
    "        upper = cat_op([mids, z_vals[..., -1:]])\n",
    "        lower = cat_op([z_vals[..., :1], mids])\n",
    "        # Stratified samples in those intervals\n",
    "        t_rand = md.numpy.rand(z_vals.shape)\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "\n",
    "    return z_vals\n",
    "\n",
    "\n",
    "def sample_pdf(bins, weights, cap_cap_n_samples, det=False):\n",
    "    \"\"\"\n",
    "    Sample pdf function.\n",
    "\n",
    "    Args:\n",
    "        bins (int): The number of bins for pdf.\n",
    "        weights (Tensor): The estimated weights.\n",
    "        cap_cap_n_samples (int): The number of points to be sampled.\n",
    "        det (bool, optional): Deterministic run or not. Default: False.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, sampled pdf tensor.\n",
    "    \"\"\"\n",
    "    weights = weights + 1e-5\n",
    "    pdf = weights / md.numpy.sum(weights, -1, keepdims=True)\n",
    "    cdf = md.numpy.cumsum(pdf, -1)\n",
    "    cdf = md.ops.Concat(-1)([md.numpy.zeros_like(cdf[..., :1]), cdf])\n",
    "\n",
    "    # Take uniform samples\n",
    "    temp_shape = cdf.shape[:-1]\n",
    "    cap_cap_n_samples_new = cap_cap_n_samples\n",
    "    temp_shape_new = list(temp_shape) + [cap_cap_n_samples_new]\n",
    "    if det:\n",
    "        u = md.numpy.linspace(0.0, 1.0, num=cap_cap_n_samples)\n",
    "        expand_op = md.ops.BroadcastTo(temp_shape_new)\n",
    "        u = expand_op(u)\n",
    "    else:\n",
    "        u = md.numpy.rand(temp_shape_new)\n",
    "\n",
    "    # Invert CDF\n",
    "    indexes = nd_searchsorted(cdf, u)\n",
    "\n",
    "    below = md.numpy.maximum(md.numpy.zeros_like(indexes - 1), indexes - 1)\n",
    "    above = md.numpy.minimum((cdf.shape[-1] - 1) * md.numpy.ones_like(indexes),\n",
    "                             indexes)\n",
    "    indexes_g = md.ops.Stack(axis=-1)([below, above])\n",
    "\n",
    "    matched_shape = (indexes_g.shape[0], indexes_g.shape[1], cdf.shape[-1])\n",
    "    gather_op = md.ops.GatherD()\n",
    "    unsqueeze_op = md.ops.ExpandDims()\n",
    "    expand_op = md.ops.BroadcastTo(matched_shape)\n",
    "    cdf_g = gather_op(expand_op(unsqueeze_op(cdf, 1)), 2, indexes_g)\n",
    "    bins_g = gather_op(expand_op(unsqueeze_op(bins, 1)), 2, indexes_g)\n",
    "\n",
    "    denom = cdf_g[..., 1] - cdf_g[..., 0]\n",
    "    denom = md.numpy.where(denom < 1e-5, md.numpy.ones_like(denom), denom)\n",
    "    t = (u - cdf_g[..., 0]) / denom\n",
    "    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "def nd_searchsorted(cdf, u):\n",
    "    \"\"\"N-dim searchsorted.\n",
    "\n",
    "    Args:\n",
    "        cdf (Tensor): The cdf sampling weights.\n",
    "        u (Tensor): The interval tensors.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, index after searchsorted ops.\n",
    "    \"\"\"\n",
    "    spatial_shape = cdf.shape[:-1]\n",
    "    last_dim_cdf, last_dim_u = cdf.shape[-1], u.shape[-1]\n",
    "    cdf_, u_ = cdf.view(-1, last_dim_cdf), u.view(-1, last_dim_u)\n",
    "    indexes_ls = []\n",
    "\n",
    "    for i in range(cdf_.shape[0]):\n",
    "        indexes_ls.append(cdf_[i].searchsorted(u_[i], side=\"right\"))\n",
    "    indexes = md.ops.Stack(axis=0)(indexes_ls)\n",
    "    indexes = indexes.view(*spatial_shape, last_dim_u)\n",
    "    return indexes\n",
    "\n",
    "train_image_list = []\n",
    "with tqdm(range(1, cap_n_iters + 1)) as pbar:\n",
    "    pbar.n = start_iter\n",
    "\n",
    "    for i in pbar:\n",
    "        # Show progress\n",
    "        pbar.set_description(f'Iter {global_steps + 1:d}')\n",
    "        pbar.update()\n",
    "\n",
    "        # Start time of the current iteration\n",
    "        time0 = time.time()\n",
    "\n",
    "        img_i = int(np.random.choice(i_train))\n",
    "\n",
    "        target = images[img_i]\n",
    "        pose = poses[img_i, :3, :4]\n",
    "\n",
    "        if cap_n_rand is not None:\n",
    "            rays_o, rays_d = generate_rays(\n",
    "                cap_h, cap_w, focal,\n",
    "                pose)  # (cap_h, cap_w, 3), (cap_h, cap_w, 3)\n",
    "            sampled_rows, sampled_cols = sample_grid_2d(\n",
    "                cap_h, cap_w, cap_n_rand)\n",
    "            rays_o = rays_o[sampled_rows, sampled_cols]  # (cap_n_rand, 3)\n",
    "            rays_d = rays_d[sampled_rows, sampled_cols]  # (cap_n_rand, 3)\n",
    "\n",
    "            batch_rays = md.ops.Stack(axis=0)([rays_o, rays_d])\n",
    "            target_s = target[sampled_rows, sampled_cols]  # (cap_n_rand, 3)\n",
    "\n",
    "        loss, psnr = train_net(global_steps, train_renderer, optimizer,\n",
    "                               batch_rays, target_s)\n",
    "\n",
    "        pbar.set_postfix(time=time.time() - time0, loss=loss, psnr=psnr)\n",
    "\n",
    "        # Save testing results\n",
    "        if (global_steps + 1) % i_testset == 0:\n",
    "            test_idx = np.random.randint(low=0, high=len(i_test))\n",
    "            test_time, test_loss, test_psnr, sub_train_image_list = test_net(\n",
    "                cap_h, cap_w, focal, renderer,\n",
    "                poses[i_test[test_idx:test_idx + 1].tolist()],\n",
    "                images[i_test[test_idx:test_idx + 1].tolist()])\n",
    "            train_image_list.extend(sub_train_image_list)\n",
    "        global_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline\n",
    "\n",
    "def showGif(image_list, name):\n",
    "    show_list = []\n",
    "    fig = plt.figure(figsize=(4, 4), dpi=120)\n",
    "    for epoch in range(len(image_list)):\n",
    "        plt.axis(\"off\")\n",
    "        show_list.append([plt.imshow(image_list[epoch])])\n",
    "\n",
    "    ani = animation.ArtistAnimation(fig, show_list, interval=1000, repeat_delay=1000, blit=True)\n",
    "    ani.save(f'images/{name}.gif', writer='pillow', fps=1)\n",
    "\n",
    "showGif(train_image_list, \"nerf.train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train.git](images/nerf.train.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 渲染"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time, test_loss, test_psnr, test_image_list = test_net(\n",
    "    cap_h,\n",
    "    cap_w,\n",
    "    focal,\n",
    "    renderer,\n",
    "    md.Tensor(poses[i_test.tolist()]),\n",
    "    images[i_test.tolist()])\n",
    "print(\n",
    "    f\"Testing results: [ Mean Time: {test_time:.4f}s, Loss: {test_loss:.4f}, PSNR: {test_psnr:.4f} ]\"\n",
    ")\n",
    "showGif(test_image_list, \"nerf.test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train.git](images/nerf.test.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推理\n",
    "\n",
    "使用预训练权重推理最佳结果。建议重启 jupyter kernel 后运行以下代码块，避免内存不足问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"GPU\"\n",
    "device_id = 0\n",
    "mode = \"GRAPH_MODE\"\n",
    "\n",
    "# 下载权重\n",
    "!if [[ ! -f ckpts/200000.blender_lego_coarse_0.ckpt ]]; then mkdir -p ckpts/ && wget -P ckpts/ https://download.mindspore.cn/vision/nerf/lego/200000.blender_lego_coarse_0.ckpt; fi\n",
    "# 运行推理脚本\n",
    "!cd src && GLOG_v=3 python eval.py --name lego_coarse --data_dir ../datasets/nerf_synthetics/lego --dataset_type blender --half_res --cap_n_rand 4096 --cap_n_samples 192 --cap_n_importance 0 --use_view_dirs --raw_noise_std 1e0 --white_bkgd --device {device} --device_id {device_id} --precision fp16 --mode {mode} --render_test --ckpt ../ckpts/200000.blender_lego_coarse_0.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline\n",
    "\n",
    "def showGif(image_list, name):\n",
    "    show_list = []\n",
    "    fig = plt.figure(figsize=(4, 4), dpi=120)\n",
    "    for epoch in range(len(image_list)):\n",
    "        plt.axis(\"off\")\n",
    "        show_list.append([plt.imshow(image_list[epoch])])\n",
    "\n",
    "    ani = animation.ArtistAnimation(fig, show_list, interval=1000, repeat_delay=1000, blit=True)\n",
    "    ani.save(f'images/{name}.gif', writer='pillow', fps=1)\n",
    "\n",
    "inference_image_list = [imageio.imread(image_path) for image_path in sorted(glob.glob(\"src/results/blender_lego_coarse_0/renderonly_200000/*.png\"))]\n",
    "showGif(inference_image_list, \"nerf.inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inference.gif](images/nerf.inference.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本案例对 NeRF 的论文中提出的模型进行了详细的解释，向读者完整地展现了该算法的流程。如需查看详细代码，可参考 `src/`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引用\n",
    "\n",
    "[1] Mildenhall, Ben et al. “NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.” ECCV (2020).\n",
    "\n",
    "### 参考资料\n",
    "\n",
    "- [NeRF 仓库](https://github.com/bmild/nerf)：官方 NeRF 的存储库。\n",
    "\n",
    "- [NeRF 视频](https://www.youtube.com/watch?v=dPWLybp4LL0)：关于 NeRF 的讲解视频。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mindspore_py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "477601a9609df58d3ea7387ec099814562b14843624f74b8476e0272e25e85a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
